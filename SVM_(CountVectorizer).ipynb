{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM (CountVectorizer).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "M0bGuZnZbr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3NkdiLqaUIv",
        "outputId": "bcf33256-2ead-4034-90eb-81e56ce2b773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd # our main data management package\n",
        "import matplotlib.pyplot as plt # our main display package\n",
        "import string # used for preprocessing\n",
        "import re # used for preprocessing\n",
        "import nltk # the Natural Language Toolkit, used for preprocessing\n",
        "import numpy as np # used for managing NaNs\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords # used for preprocessing\n",
        "from nltk.stem import WordNetLemmatizer # used for preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression # our model\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, classification\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/news.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "msBpPfq1af40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().any().any()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmF8MQIibsAA",
        "outputId": "388406f0-5591-4a8a-a8f1-5e93847e74f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove urls, handles, and the hashtag from hashtags (taken from https://stackoverflow\n",
        "def remove_urls(text):\n",
        " new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text))\n",
        " return new_text\n",
        "# make all text lowercase\n",
        "def text_lowercase(text):\n",
        " return text.lower()\n",
        "# remove numbers\n",
        "def remove_numbers(text):\n",
        " result = re.sub(r'\\d+', '', text)\n",
        " return result\n",
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        " translator = str.maketrans('', '', string.punctuation)\n",
        " return text.translate(translator)\n",
        "# tokenize\n",
        "def tokenize(text):\n",
        " text = word_tokenize(text)\n",
        " return text\n",
        "# remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        " text = [i for i in text if not i in stop_words]\n",
        " return text\n",
        "# lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(text):\n",
        " text = [lemmatizer.lemmatize(token) for token in text]\n",
        " return text\n",
        "def preprocessing(text):\n",
        " text = text_lowercase(text)\n",
        " text = remove_urls(text)\n",
        " text = remove_numbers(text)\n",
        " text = remove_punctuation(text)\n",
        " text = tokenize(text)\n",
        " text = remove_stopwords(text)\n",
        " text = lemmatize(text)\n",
        " text = ' '.join(text)\n",
        " return text\n"
      ],
      "metadata": {
        "id": "KBicKxjmbr8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp_text_train = [] # our preprocessed text column\n",
        "for text_data in df['text']:\n",
        " pp_text_data = preprocessing(text_data)\n",
        " pp_text_train.append(pp_text_data)\n",
        "df['pp_text'] = pp_text_train # add the preprocessed text as a column"
      ],
      "metadata": {
        "id": "sRmTfEybbr7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp_title_train = [] # our preprocessed text column\n",
        "for title_data in df['title']:\n",
        " pp_title_data = preprocessing(title_data)\n",
        " pp_title_train.append(pp_title_data)\n",
        "df['pp_title'] = pp_title_train # add the preprocessed text as a column"
      ],
      "metadata": {
        "id": "AqvOQ_Bzbrry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['text']\n",
        "df.rename({'Unnamed: 0': 'id'}, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "s7fyMYlEbrgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['title']\n"
      ],
      "metadata": {
        "id": "k6G1-JwQbrSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "9oFtxss8fM39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Get the labels\n",
        "labels=df.label\n",
        "labels"
      ],
      "metadata": {
        "id": "pyg9sNAyfNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Split the dataset\n",
        "x_train,x_test,y_train,y_test=train_test_split(df['pp_text'], labels, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "Tr0yhUFUfQ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "SKyluzsrlVZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', max_df=0.7)\n",
        "X = vectorizer.fit_transform(x_train)\n",
        "#X = X.toarray()\n",
        "tfidf_test=vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "B152gwhsfX0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "svc = SVC(kernel='linear', C = 1.0)\n",
        "svc.fit(tfidf_train, y_train)\n",
        "pred_svc = svc.predict(tfidf_test)\n",
        "parameters = svc.coef_\n",
        "accuracy = metrics.accuracy_score(y_test,pred_svc)\n",
        "print('accuracy for data',accuracy*100)\n"
      ],
      "metadata": {
        "id": "m-3SjPyzfagm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = metrics.precision_score(y_test,pred_svc)\n",
        "print('precision:',pre*100,'%')\n",
        "re = metrics.recall_score(y_test,pred_svc)\n",
        "print('recall:',re*100,'%')\n",
        "f1 = metrics.f1_score(y_test,pred_svc)\n",
        "print('f1_score:',f1*100,'%')\n"
      ],
      "metadata": {
        "id": "SmthYAUKswVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix=pd.crosstab(y_test,predictions,rownames=['Actual'],colnames=['predicted'])"
      ],
      "metadata": {
        "id": "Qw7CcZ3Kfeih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('standard errors:',np.sqrt(np.diag(confusion_matrix)))"
      ],
      "metadata": {
        "id": "8REyrwe-fefi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['FAKE', 'REAL'], yticklabels=['FAKE', 'REAL'], cmap=plt.cm.Blues,cbar=False)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ebGZs-kfh8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "NJ8PQrvZfhx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}